{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpecAugment\n",
    "\n",
    "Spectrogram Augmentation Paper\n",
    "\n",
    "operates on the log mel spectrogram of the input audio, rather than the raw audio itself.\n",
    "\n",
    "it directly acts on the log mel spectrogram as if it were an image, and does not require any additional data.\n",
    "\n",
    "SpecAugment consists of three kinds of deformations of the log mel spectrogram. \"The first is time warping, a deformation of the time-series in the time direction.\" The other two augmentations, inspired by \"Cutout\", proposed in computer vision[T. DeVries and G. Taylor \"Improved Regularization of Convolutional Neural Networks with Cutout\" in arXiv, 2017], are time and frequency masking, where we mask a block of consecutive time stops or mel frequency channels.\n",
    "\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Warping \n",
    "\n",
    "-------------\n",
    "\n",
    "Time Warping is applied via the function 'sparse_image_warp' of 'tensorflow'. Given a log mel spectrogram with $\\tau$ time steps. A random point along the time axis (W, t - W) is to be warped either to the left or right by a distance 'w' chosen from a uniform distribution from 0 to the time warp parameter W along that line.\n",
    "\n",
    "W : random point    \n",
    "t : length of spectrogram\n",
    "\n",
    "a : random start point   \n",
    "b : destination point   \n",
    "\n",
    "(처음 데이터)   \n",
    "0 - ############################################################################ - t  (Original)<br>\n",
    "(0부터 W 사이의 임의의 값 랜덤을 w에 지정), (양쪽을 w크기 만큼 줄임)  \n",
    "0 - 00000w#############################################################t-w000000 - t  (Random range, w (0,W))<br>\n",
    "(w ~ t-w 사이의 임의의 a 지정)   \n",
    "0 - 00000w###############################a#############################t-w000000 - t  (a in (W ~ t-W))<br>\n",
    "(a 주변으로 w 거리)   \n",
    "0 - 00000w###########a-w#################a###############a+w###########t-w000000 - t  ()<br>\n",
    "(사이의 임의의 b점 지정)   \n",
    "0 - 00000w###########a-w#####b###########a###############a+w###########t-w000000 - t  ()<br>\n",
    "(a점에서 b점으로 sparse_images_warp)    \n",
    "0 - 00000w###########a-w#####b##<-warp-##a###############a+w###########t-w000000 - t  ()<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code    \n",
    "from [github.DemisEom/SpecAugment](https://github.com/DemisEom/SpecAugment/blob/master/SpecAugment/sparse_image_warp_np.py)\n",
    "\n",
    "plus+ [zcaceres/spec_augment](https://github.com/zcaceres/spec_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import skimage\n",
    "from scipy.interpolate import interp2d\n",
    "from skimage.transform import warp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get grid locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_grid_locations(image_height, image_width):\n",
    "  \"\"\"Wrapper for np.meshgrid.\"\"\"\n",
    "\n",
    "  y_range = np.linspace(0, image_height - 1, image_height)\n",
    "  x_range = np.linspace(0, image_width - 1, image_width)\n",
    "  y_grid, x_grid = np.meshgrid(y_range, x_range, indexing='ij')\n",
    "  return np.stack((y_grid, x_grid), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand to minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_to_minibatch(np_array, batch_size):\n",
    "  \"\"\"Tile arbitrarily-sized np_array to include new batch dimension.\"\"\"\n",
    "  tiles = [batch_size] + [1] * np_array.ndim\n",
    "  return np.tile(np.expand_dims(np_array, 0), tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get boundary locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_boundary_locations(image_height, image_width, num_points_per_edge):\n",
    "  \"\"\"Compute evenly-spaced indices along edge of image.\"\"\"\n",
    "  y_range = np.linspace(0, image_height - 1, num_points_per_edge + 2)\n",
    "  x_range = np.linspace(0, image_width - 1, num_points_per_edge + 2)\n",
    "  ys, xs = np.meshgrid(y_range, x_range, indexing='ij')\n",
    "  is_boundary = np.logical_or(\n",
    "      np.logical_or(xs == 0, xs == image_width - 1),\n",
    "      np.logical_or(ys == 0, ys == image_height - 1))\n",
    "  return np.stack([ys[is_boundary], xs[is_boundary]], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add zero flow controls at boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_zero_flow_controls_at_boundary(control_point_locations,\n",
    "                                        control_point_flows, image_height,\n",
    "                                        image_width, boundary_points_per_edge):\n",
    "\n",
    "  # batch_size = tensor_shape.dimension_value(control_point_locations.shape[0])\n",
    "  batch_size = control_point_locations.shape[0]\n",
    "\n",
    "  boundary_point_locations = _get_boundary_locations(image_height, image_width,\n",
    "                                                     boundary_points_per_edge)\n",
    "\n",
    "  boundary_point_flows = np.zeros([boundary_point_locations.shape[0], 2])\n",
    "\n",
    "  type_to_use = control_point_locations.dtype\n",
    "  # boundary_point_locations = constant_op.constant(\n",
    "  #     _expand_to_minibatch(boundary_point_locations, batch_size),\n",
    "  #     dtype=type_to_use)\n",
    "  boundary_point_locations = _expand_to_minibatch(boundary_point_locations, batch_size)\n",
    "\n",
    "  # boundary_point_flows = constant_op.constant(\n",
    "  #     _expand_to_minibatch(boundary_point_flows, batch_size), dtype=type_to_use)\n",
    "  boundary_point_flows = _expand_to_minibatch(boundary_point_flows, batch_size)\n",
    "\n",
    "  # merged_control_point_locations = array_ops.concat(\n",
    "  #     [control_point_locations, boundary_point_locations], 1)\n",
    "\n",
    "  merged_control_point_locations = np.concatenate(\n",
    "      [control_point_locations, boundary_point_locations], 1)\n",
    "\n",
    "  # merged_control_point_flows = array_ops.concat(\n",
    "  #     [control_point_flows, boundary_point_flows], 1)\n",
    "\n",
    "  merged_control_point_flows = np.concatenate(\n",
    "      [control_point_flows, boundary_point_flows], 1)\n",
    "\n",
    "  return merged_control_point_locations, merged_control_point_flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparese Image Warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_image_warp_np(image,\n",
    "                      source_control_point_locations,\n",
    "                      dest_control_point_locations,\n",
    "                      interpolation_order=2,\n",
    "                      regularization_weight=0.0,\n",
    "                      num_boundary_points=0):\n",
    "\n",
    "  # image = ops.convert_to_tensor(image)\n",
    "  # source_control_point_locations = ops.convert_to_tensor(\n",
    "  #     source_control_point_locations)\n",
    "  # dest_control_point_locations = ops.convert_to_tensor(\n",
    "  #     dest_control_point_locations)\n",
    "\n",
    "  control_point_flows = (\n",
    "      dest_control_point_locations - source_control_point_locations)\n",
    "\n",
    "  clamp_boundaries = num_boundary_points > 0\n",
    "  boundary_points_per_edge = num_boundary_points - 1\n",
    "\n",
    "  # batch_size, image_height, image_width, _ = image.get_shape().as_list()\n",
    "  batch_size, image_height, image_width, _ = list(image.shape)\n",
    "\n",
    "  # This generates the dense locations where the interpolant\n",
    "  # will be evaluated.\n",
    "\n",
    "  grid_locations = _get_grid_locations(image_height, image_width)\n",
    "\n",
    "  flattened_grid_locations = np.reshape(grid_locations,\n",
    "                                          [image_height * image_width, 2])\n",
    "\n",
    "    # flattened_grid_locations = constant_op.constant(\n",
    "    #     _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\n",
    "\n",
    "  flattened_grid_locations = _expand_to_minibatch(flattened_grid_locations, batch_size)\n",
    "\n",
    "  if clamp_boundaries:\n",
    "    (dest_control_point_locations,\n",
    "     control_point_flows) = _add_zero_flow_controls_at_boundary(\n",
    "         dest_control_point_locations, control_point_flows, image_height,\n",
    "         image_width, boundary_points_per_edge)\n",
    "\n",
    "    # flattened_flows = interpolate_spline.interpolate_spline(\n",
    "    #     dest_control_point_locations, control_point_flows,\n",
    "    #     flattened_grid_locations, interpolation_order, regularization_weight)\n",
    "  flattened_flows = sp.interpolate.spline(\n",
    "        dest_control_point_locations, control_point_flows,\n",
    "        flattened_grid_locations, interpolation_order, regularization_weight)\n",
    "\n",
    "    # dense_flows = array_ops.reshape(flattened_flows,\n",
    "    #                                 [batch_size, image_height, image_width, 2])\n",
    "  dense_flows = np.reshape(flattened_flows,\n",
    "                                    [batch_size, image_height, image_width, 2])\n",
    "\n",
    "    # warped_image = dense_image_warp.dense_image_warp(image, dense_flows)\n",
    "  warped_image = warp(image, dense_flows)\n",
    "\n",
    "  return warped_image, dense_flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense image warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_image_warp(image, flow):\n",
    "    # batch_size, height, width, channels = (array_ops.shape(image)[0],\n",
    "    #                                        array_ops.shape(image)[1],\n",
    "    #                                        array_ops.shape(image)[2],\n",
    "    #                                        array_ops.shape(image)[3])\n",
    "    batch_size, height, width, channels = (np.shape(image)[0],\n",
    "                                           np.shape(image)[1],\n",
    "                                           np.shape(image)[2],\n",
    "                                           np.shape(image)[3])\n",
    "\n",
    "    # The flow is defined on the image grid. Turn the flow into a list of query\n",
    "    # points in the grid space.\n",
    "    # grid_x, grid_y = array_ops.meshgrid(\n",
    "    #     math_ops.range(width), math_ops.range(height))\n",
    "    # stacked_grid = math_ops.cast(\n",
    "    #     array_ops.stack([grid_y, grid_x], axis=2), flow.dtype)\n",
    "    # batched_grid = array_ops.expand_dims(stacked_grid, axis=0)\n",
    "    # query_points_on_grid = batched_grid - flow\n",
    "    # query_points_flattened = array_ops.reshape(query_points_on_grid,\n",
    "    #                                            [batch_size, height * width, 2])\n",
    "    grid_x, grid_y = np.meshgrid(\n",
    "        np.range(width), np.range(height))\n",
    "    stacked_grid = np.cast(\n",
    "        np.stack([grid_y, grid_x], axis=2), flow.dtype)\n",
    "    batched_grid = np.expand_dims(stacked_grid, axis=0)\n",
    "    query_points_on_grid = batched_grid - flow\n",
    "    query_points_flattened = np.reshape(query_points_on_grid,\n",
    "                                        [batch_size, height * width, 2])\n",
    "    # Compute values at the query points, then reshape the result back to the\n",
    "    # image grid.\n",
    "    interpolated = interp2d(image, query_points_flattened)\n",
    "    interpolated = np.reshape(interpolated,\n",
    "                              [batch_size, height, width, channels])\n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_warp(spec, W=5):\n",
    "    num_rows = spec.shape[1]\n",
    "    spec_len = spec.shape[2]\n",
    "\n",
    "    y = num_rows // 2\n",
    "    horizontal_line_at_ctr = spec[0][y]\n",
    "    # assert len(horizontal_line_at_ctr) == spec_len\n",
    "\n",
    "    point_to_warp = horizontal_line_at_ctr[random.randrange(W, spec_len-W)]\n",
    "    # assert isinstance(point_to_warp, torch.Tensor)\n",
    "\n",
    "    # Uniform distribution from (0,W) with chance to be up to W negative\n",
    "    dist_to_warp = random.randrange(-W, W)\n",
    "    src_pts = torch.tensor([[[y, point_to_warp]]])\n",
    "    dest_pts = torch.tensor([[[y, point_to_warp + dist_to_warp]]])\n",
    "    warped_spectro, dense_flows = sparse_image_warp(spec, src_pts, dest_pts)\n",
    "    return warped_spectro.squeeze(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Masking\n",
    "---------------------------------\n",
    "Frequency masking is applied so that f consecutive mel frequency channels (f0, f0 + f) are masked, where f is first chosen from a uniform distribution from 0 to the frequency mask parameter F, and f0 is chosen from (0, v - f). v is the number of mel frequency channels.\n",
    "\n",
    "f0 : random frequency point   \n",
    "f  : masking length    \n",
    "v  : end-point of frequency    \n",
    "F  : Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Masking \n",
    "----------------------------------\n",
    "Time masking is applied so that t consecutive time steps (t0, t0 + t) are masked, where t is first chosen from a uniform distribution form 0 to the time mask parameter T, and t0 is chosen from (0, $\\tau$ - t).\n",
    "\n",
    "t0 : random time point   \n",
    "t : masking length   \n",
    "$\\tau$ : endpoint of time   \n",
    "T : Parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
